<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Marco Banterle" />

<meta name="date" content="2018-10-11" />

<title>rBSEM - Bayesian Structural Equation Models</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">rBSEM - Bayesian Structural Equation Models</h1>
<h4 class="author"><em>Marco Banterle</em></h4>
<h4 class="date"><em>2018-10-11</em></h4>



<!-- annoying that i need to redefine thecommand inside latex environment, but... -->
<span class="math display">\[\begin{align} \newcommand{\bms}[1]{\boldsymbol{#1}} \end{align}\]</span>
<div id="model-specification" class="section level1">
<h1>Model Specification</h1>
<p>This software performs Bayesian inference through MCMC on the following structural equation regression model:</p>
<p>All variables which appear as outcomes in a regression equation (endogenous variables) are denoted by <span class="math inline">\(y\)</span>. These can also appear as predictors. Variables which only appear as predictors (exogenous variables) are denoted by <span class="math inline">\(x\)</span>.</p>
<p>For one individual, we will have a vector of endogenous variables (outcomes) <span class="math inline">\({\boldsymbol{y}}_i\)</span>, length <span class="math inline">\(s\)</span>. Each outcome <span class="math inline">\({\boldsymbol{y}}_{i,k}\)</span> can be a vector of <span class="math inline">\(s_k\)</span> variables and will denote the single univariate component as <span class="math inline">\(y_{i,k,l}\)</span>, for individual <span class="math inline">\(i\)</span>, outcome <span class="math inline">\(k\)</span> and component <span class="math inline">\(l\)</span>. We will denote the error terms <span class="math inline">\(\epsilon\)</span>, <span class="math inline">\({\boldsymbol{\epsilon}}_i\)</span>, <span class="math inline">\({\boldsymbol{\epsilon}}_{i,k}\)</span> and <span class="math inline">\(\epsilon_{i,k,l}\)</span> in a similar fashion.</p>
<p>For each outcome <span class="math inline">\(k = 1,...,s\)</span> we define a vector of predictor variables <span class="math inline">\({\boldsymbol{x}}_{i,k}\)</span>, with length <span class="math inline">\(p_k\)</span>, where each individual element will be denoted <span class="math inline">\({x}_{i,k,j}\)</span> for individual <span class="math inline">\(i\)</span>, outcome <span class="math inline">\(k\)</span> and component <span class="math inline">\(j\)</span>.</p>
<p>We will also denote <span class="math inline">\({\boldsymbol{y}}_k\)</span> <span class="math inline">\({\boldsymbol{\epsilon}}_k\)</span> and <span class="math inline">\({\boldsymbol{x}}_k\)</span> the corresponding matrices (<span class="math inline">\(n \times s_k\)</span>) for all individuals.</p>
<p>An example of a structural equation model with 2 endogenous (possibly multivariate) variables is</p>
<span class="math display">\[\begin{align}\label{eq:example_2var}
\bms y_{i,2} &amp;= \lambda_2 \bms y_{i,1} +  (1, \bms {x}_{i,2}^t ) \bms {\beta}_{2} + \bms \epsilon_{i,2} \nonumber\\
\bms y_{i,1 }&amp;=   (1, \bms {x}_{i,1}^t) \bms {\beta}_{1} + \bms \epsilon_{i,1}
\end{align}\]</span>
<p>where the intercept coefficients are included in the <span class="math inline">\({\boldsymbol{\beta}}_k\)</span> vectors (so <span class="math inline">\({\boldsymbol{\beta}}_k\)</span> is length <span class="math inline">\(p_k+1\)</span>).</p>
We can rewrite this as
<span class="math display">\[\begin{align*}
\begin{pmatrix}
1 &amp; -\lambda_2 \\
0 &amp; 1
\end{pmatrix}
\begin{pmatrix}
\bms y_{i,2} \\
\bms y_{i,1}
\end{pmatrix}
=
\begin{pmatrix}
(1, \bms{x}_{i,2}^t) &amp; 0 \\
0 &amp; (1, \bms{x}_{i,1}^t) 
\end{pmatrix}
\begin{pmatrix}
\bms{\beta}_{2} \\
\bms{\beta}_{1} 
\end{pmatrix}
+
\begin{pmatrix}
\bms\epsilon_{i,2} \\
\bms\epsilon_{i,1}
\end{pmatrix}
\end{align*}\]</span>
<p>or more concisely</p>
<span class="math display">\[\begin{align*} 
\Lambda \bms{y}_i = X_i \bms{\beta} + \bms{\epsilon}_i
\end{align*}\]</span>
<p>where <span class="math inline">\(\Lambda\)</span> is <span class="math inline">\(s \times s\)</span> and <span class="math inline">\({\boldsymbol{y}}_i\)</span> and <span class="math inline">\({\boldsymbol{\epsilon}}_i\)</span> are length <span class="math inline">\(s\)</span>.</p>
<p>The regression coefficients are bundled together and (in this example) include the intercept terms for all outcomes. The design matrix <span class="math inline">\(X_i\)</span> is thus <span class="math inline">\(n \times (\sum_k p_k+s)\)</span>.</p>
<p>Some elements of <span class="math inline">\(\Lambda\)</span> will be set to be zero, as in the example above, both to define the structure of the model and to ensure identifiability. The maximum number of non-zero elements of <span class="math inline">\(\Lambda\)</span> is assumed thus to be <span class="math inline">\(s(s-1)/2\)</span> and only the upper diagonal can be populated, so that no symmetry (or no <em>feedback</em> as they’re usually called in econometrics) is possible.</p>
<p>We will also make the simplifying assumption of independent <em>residuals</em> between the different outcomes, so we set <span class="math inline">\({\boldsymbol{\epsilon}}_{i,k} \sim N({\boldsymbol{0}}, R_k)\)</span> where <span class="math inline">\(R_k\)</span> is a <span class="math inline">\(s_k \times s_k\)</span> dimensional <em>diagonal</em> covariance matrix, <em>i.e.</em> <span class="math inline">\(\mathbb{E}[\epsilon_{i,k,l},\epsilon_{i,k,l'}] = 0\)</span>, and we’ll also assume independence between components in different outcomes, <em>i.e.</em> <span class="math inline">\(\mathbb{E}[{\boldsymbol{\epsilon}}_{i,k},{\boldsymbol{\epsilon}}_{i,k'}] = {\boldsymbol{0}}\)</span>.</p>
<p>For a more in-depth introduction to SEMs see for example .</p>
<p>Call <span class="math inline">\(A_{i,k}\)</span> for <span class="math inline">\(k \in \{1,\dots,s\}\)</span> the vector of predictors for outcome <span class="math inline">\(k\)</span>, <em>i.e.</em> <span class="math inline">\(A_{i,2} = \left( 1, {\boldsymbol{x}}_{i,2} , {\boldsymbol{y}}_{i,1} \right)\)</span> in the above example, and <span class="math inline">\(B_k = \left(\beta_k,\lambda_k\right)\)</span> their corresponding regression coefficients associated with <span class="math inline">\({\boldsymbol{y}}_{i,k}\)</span>.</p>
<p>In general let’s define <span class="math inline">\(I_k \subseteq \{ 1, \dots, s \} \setminus k\)</span> the set indexes for outcomes that appear on the right-hand side of the regression equation <span class="math inline">\(k\)</span>; in the above example we have <span class="math inline">\(I_1 = \emptyset\)</span> and <span class="math inline">\(I_2 = \{1\}\)</span>. This can be seen as the non-zero values in column <span class="math inline">\(k\)</span> of the matrix <span class="math inline">\(\Lambda\)</span> above, excluding the diagonal. Then we have that <span class="math inline">\(B_k\)</span> has dimension <span class="math inline">\(s_k \times p_k + \sum \limits_{j \in {I}_k} s_j\)</span>.</p>
<div id="variable-selection" class="section level2">
<h2>Variable Selection</h2>
<p>We want to perform variable selection on all these predictors in a general way, such that each component in <span class="math inline">\({\boldsymbol{y}}_k\)</span> can be associated with a potentially different set of predictors in <span class="math inline">\(A_{\cdot,k}\)</span>.</p>
<p>We achieve this by introducing a set of binary matrices <span class="math inline">\(\Gamma_k\)</span>, with similar dimensions to <span class="math inline">\(B_k\)</span>, that effectively multiply each regression coefficient making so that only variables whose <span class="math inline">\(\gamma\)</span> is different from zero will impact the outcome.</p>
<p>As it is common to have a subset of the covariates that we do not want to exclude from the model, we will allow for some covariates to be considered at all times; we will call these predictors <em>required</em> and we will essentially fix their <span class="math inline">\(\gamma\)</span> coefficients to <span class="math inline">\(1\)</span>; the intercept for example is usually considered to always be included. In order to distinguish from <em>required</em> variables, we will call the ones whose <span class="math inline">\(\gamma\)</span> coefficient is random <em>optional</em> and we note here that each outcome variable appearing on the right-hand-side will always be considered <em>optional</em>.</p>
<p>By writing <span class="math inline">\(B_{\gamma_k}\)</span> and <span class="math inline">\(A_{\cdot,\gamma_k}\)</span> the set of non-zero regression coefficients and their corresponding associated covariates we can thus re-write the model as</p>
<span class="math display">\[\begin{equation}\label{eq:SEM_VS}
\bms y_{i,k} = B_{\gamma_k}A_{i,\gamma_k} + \bms \epsilon_{i,k} \quad\quad k=1,\dots,s
\end{equation}\]</span>
<p>The parameters <span class="math inline">\(\Gamma_k\)</span> are the main object of inference, while every other variable will be integrated out analytically thanks to conjugacy. See for example  for more details on this type of variable selection model.</p>
</div>
<div id="prior-specification" class="section level2">
<h2>Prior Specification</h2>
<p>We want to take advantage of conjugacy as much as possible. Each diagonal coefficient of the residual matrices is distributed according to an Inverse Gamma <span class="math display">\[[R_k]_{l,l} \sim i\Gamma\left(a_{k,l},b_{k,l}\right)\]</span> The regression coefficients are Normally distributed (remember <span class="math inline">\(B_k\)</span> is a matrix <span class="math inline">\(s_k \times p_k + \sum \limits_{j \in \mathcal{S}_k} s_j\)</span>) <span class="math display">\[ vec\left(B_k\right) \sim \mathcal{N}\left( {\boldsymbol{0}} , I_{s_k} \otimes W_{0,k} \right)\]</span> with <span class="math inline">\(W_{0,k}\)</span> a general valid covariance matrix; this formalism encompasses both a completely independent hyper-parameter, a residual-dependent covariance like <span class="math inline">\(\frac{R_k}{w}\)</span> or even an empirical-Bayes-type <span class="math inline">\(g\)</span>-priors like <span class="math inline">\(g \left( A_k^tA_k\right)^{-1}\)</span>. We are assuming that <span class="math inline">\(B_{k,l}\)</span>, the vector of all the coefficients for component <span class="math inline">\(l\)</span> of outcome <span class="math inline">\(k\)</span> is independent from <span class="math inline">\({\boldsymbol{B}}_{k,l'}\)</span> for each <span class="math inline">\(l' \in \{1,\dots,s_k\} \setminus l\)</span>.</p>
<p>This leads (conditionally on all the <span class="math inline">\(\Gamma_k\)</span>s) to an overall multivariate <span class="math inline">\(t\)</span>-distributed marginal likelihood <span class="math inline">\(p(y|\Gamma_{1:s})\)</span> which is analytically tractable. See again  for more details.</p>
<p>Each coefficient in each of the <span class="math inline">\(\Gamma_k\)</span> follows <em>a priori</em> a Bernoulli distribution <span class="math inline">\(\gamma_{k,l,j} \sim \mathcal{B}er( \omega_{k,l,j} )\)</span> where <span class="math inline">\(\omega_{k,l,j} \sim \mathcal{B}eta( a_{j} ,b_{j} )\)</span>; this allows us to link inference in a hierarchical way, assuming a sort propensity for a given predictor to be included regardless of the associated outcome, pooling information from all outcomes to help in inferring the variable selection procedure. Other priors are possible, for example  use <span class="math inline">\(\omega_{k,l,j} = \rho_l \nu_j\)</span>, pooling both across predictors and outcomes.</p>
<p><em>All these priors are hard-coded for the moment to sort of default values that depends on the dimensionality of the problem, would it be interesting to have them as potential input of the software as well to be able to tweak them?</em></p>
</div>
<div id="data-imputation" class="section level2">
<h2>Data Imputation</h2>
<p>As it is often the case that interesting data present missing data, the software has (albeit limited) capability to impute them via a Data Augmentation type of algorithm, with the main assumption that the data are <em>missing at random</em> (i.e. there’s no systematic/probabilistic mechanism that censor the data based on the other variables under analysis).</p>
<p>The type of variables can be both continuous, binary or ordinal and this will need to be hinted to the software at the start of the procedure (see below).</p>
</div>
<div id="sampler-specification" class="section level2">
<h2>Sampler Specification</h2>
<p>The MCMC algorithm we’re using is based on the Evolutionary Stochastic Search MCMC of  and related works. We thus run multiple chains in parallel and perform global exchange/cross-over moves between chains to aid the mixing in the binary <span class="math inline">\(\Gamma\)</span> space.</p>
<p>The proposal distribution for the binary coefficients in <span class="math inline">\(\Gamma_k\)</span> is either a more traditional random add-delete move for a single coefficient at a time (named <span class="math inline">\(MC3\)</span> in ) or a novel adaptive proposal distribution.</p>
</div>
</div>
<div id="software-usage" class="section level1">
<h1>Software Usage</h1>
<p>In order to install the software run from R</p>
<pre><code>install.packages(&quot;[/PATH/TO]rBSEM_0.1.0.tar.gz&quot;,repos = NULL,type=&quot;source&quot;)</code></pre>
<p>The main function in the package is <code>rHESS_SEM</code>. As the name hints it is a SEM implementation of a Hierarchical ESS algorithm that sample from the posterior for the model explained above. In order to get minimum information about its functioning you can check the (developmental) documentation page using</p>
<pre><code>library(rBSEM)
?rHESS_SEM</code></pre>
<p>but more details are provided here in this vignette for now.</p>
<p>Access to this document (in HTML format) is also possible via <code>browseVignettes(&quot;rBSEM&quot;)</code> directly from <code>R</code> once the package has been installed.</p>
<div id="data-generation-and-software-input" class="section level2">
<h2>Data Generation and Software Input </h2>
<p>The software expects a simple <code>.txt</code> file containing all the variables involved in the model, where each column is, <em>with no distinction for the moment</em>, either an outcome <span class="math inline">\({y}_{1:n,k,l}\)</span>, <span class="math inline">\(k \in \{1,\dots,s\}\)</span> and <span class="math inline">\(l \in \{1,\dots,s_k\}\)</span>, or <span class="math inline">\({x}_{1:n,k,j}\)</span>, <span class="math inline">\(k \in \{1,\dots,s\}\)</span>, <span class="math inline">\(j \in \{1,\dots,p_k\}\)</span> where <span class="math inline">\(1:n\)</span> denotes the observations for each individual from <span class="math inline">\(1\)</span> to <span class="math inline">\(n\)</span> of that variable. Note that we assume that each column has the same number of rows, equal to the total number of observation <span class="math inline">\(n\)</span>.</p>
<p>Missing data should be specified as <code>NAN</code> , for example via the <code>na = &quot;NAN&quot;</code> parameter in R <code>write.table</code> function.</p>
<p>The preferred way to build the structure of the model is to introduce the concept of a <em>block</em>. We will call a block a set of variables, again with no distinction between outcomes and predictors for now.</p>
<hr />
<p>Call <span class="math inline">\(\mathcal{B} = \{b_1, \dots, b_M\}\)</span> the set of <em>blocks</em>, where each block <span class="math inline">\(b_m\)</span> has the following properties:</p>
<ul>
<li>each <span class="math inline">\(b_m\)</span> is unique, <em>i.e.</em> <span class="math inline">\(b_m \cap\)</span> <span class="math inline">\(\left( \cup_{ m' \in \left\{1:M \setminus m\right\}} b_{m'} \right)\)</span> <span class="math inline">\(= \varnothing\)</span>;</li>
<li>each <span class="math inline">\(b_m\)</span> is composed only of <span class="math inline">\(x\)</span>s of <span class="math inline">\(y\)</span>s, <em>i.e.</em> each block is only made up only by covariates or by variables that appear as outcomes in at least one equation;</li>
<li>each block composed by variables that are only covariates ( <span class="math inline">\(x\)</span>s ) is entirely made up by <em>required</em> or <em>optional</em> variables (See the Variable Selection section for details);</li>
<li>each regression equation in the SEM will have one block on the left-hand-side and any number of blocks on the right-hand-side, this means that when building the SEM we will have <span class="math display">\[[b_m = ] {\boldsymbol{y}}_{i,k} = {\boldsymbol{\epsilon}}_{i,k} + A_{i,k}B_k \left[ = \left(\sum \limits_{\mathcal{I}_m} b_{m'}\right)B_k + {\boldsymbol{\epsilon}}_{i,k}\right]\]</span> where <span class="math inline">\(\mathcal{I}_m\)</span> is the set of all the blocks that form the right-hand-side of the equation where <span class="math inline">\(b_m\)</span> appears on the left-hand-side, <em>i.e.</em> <span class="math inline">\(A_{i,k}\)</span>.</li>
</ul>
<p>Note that this does not impose that each block appear only once in the SEM and in fact it will be common to have a recurring <em>required</em> covariates block on the right-hand-side of multiple equations.</p>
<p>In order to assign each variable to the correct block in the SEM and to input the SEM structure itself we will now introduce two extra parameter needed by the software:</p>
<ul>
<li>an <code>R</code> list called <code>blockList</code> of vector of indexes that defines <span class="math inline">\(\mathcal{B}\)</span>;</li>
<li>an adjacency matrix <code>SEMGraph</code> that represent the structure of the SEM.</li>
</ul>
<p>Each element of <code>blockList</code>, <em>i.e.</em> each vector of indexes, defines one block <span class="math inline">\(b_m\)</span> by specifying the correspondent column-indexes in the data file, where <span class="math inline">\(m\)</span> is simply the order in which they appear in the list. All the columns in the data file whose index is not specified in <code>blockList</code> will simply be disregarded.</p>
<p><code>SEMGraph</code> is an <span class="math inline">\(M \times M\)</span> matrix with zeros on the diagonal where each non-zero element <span class="math inline">\(i,j\)</span> implies an edge in the SEM from block <span class="math inline">\(b_i\)</span> to block <span class="math inline">\(b_j\)</span>, which means that <span class="math inline">\(b_i \in \mathcal{I}_j\)</span>.</p>
<p>Because of the above constraints on the blocks we will have that <code>SEMGraph</code> is a completely asymmetric matrix ( if <code>SEMGraph</code><span class="math inline">\(_{(i,j)} \neq 0\)</span> then <code>SEMGraph</code><span class="math inline">\(_{(j,i)} = 0\)</span> ) and we will be able to read the SEM equations by going through <code>SEMGraph</code> column-wise: column <span class="math inline">\(j\)</span> represent the equation where <span class="math inline">\(b_j\)</span> is on the left-hand-side and every non-zero element will be a predictor; all the <span class="math inline">\({\boldsymbol{0}}\)</span> columns represent covariates-only blocks.</p>
<p>To connect back to the mathematical notation of the first Section, note that the order of the equations (<span class="math inline">\(k \in 1,\dots, s\)</span>) is in fact given by the order of the non-<span class="math inline">\({\boldsymbol{0}}\)</span> columns in <code>SEMGraph</code>; similarly the order on which the variables appear on the right-hand-side of each equation is the order of the rows in <code>SEMGraph</code> and the respective order in each <span class="math inline">\({\boldsymbol{y}}_k\)</span> or <span class="math inline">\({\boldsymbol{x}}_k\)</span> is given by their appearance in their block in <code>blockList</code>.</p>
<p>Finally note that in order to allow for distinction between <em>required</em> and <em>optional</em> predictors, each edge in the adjacency matrix will need to be specified as a + <code>1</code> for <em>optional</em> variables and + <code>2</code> for <em>required</em> variables.</p>
<hr />
<p>In order to aid the imputation algorithm a further vector <code>varType</code> of “<em>type</em> labels” is required, coded as ‘<span class="math inline">\(0\)</span>’ for continuous variables, ‘<span class="math inline">\(1\)</span>’ for binary and ‘<span class="math inline">\(2\)</span>’ for ordinal variables; the software will deduce the number of levels for ordinal variables from the data, assuming the two extremes are observed.</p>
<p><code>varType</code> will have to have length equal to either the number of columns of the input data file or the total number of indexes provided in the <code>blockList</code> argument, but <code>varType[i]</code> will always refer to the <span class="math inline">\(i^{th}\)</span> <em>used</em> column of the data matrix and will not depends on the <em>order</em> in which the variables appear in the block list.</p>
<p>For all these parameters some sanity checks are in place and should give an indication of what changes are needed in order for the inputs to be correct.</p>
<div id="autoaddintercept-gammainit-and-method-arguments" class="section level3">
<h3><code>autoAddIntercept</code>, <code>gammaInit</code> and <code>method</code> arguments</h3>
<p>Generally, unless all the outcomes have been standardized, we’d like to always include an intercept term for each equation. This translates to adding to each <span class="math inline">\(A_{1:n,k}\)</span> a column of <code>1</code>s. In the software this can also be accomplished by creating one such column in the data matrix, and create a by-itself <em>required</em> block of only one variable that connects to each outcome. By specifying <code>autoAddIntercept = TRUE</code> while calling the <code>rHESS_SEM</code> function the software will automatically take care of that,but a user requiring more flexibility (perhaps because only some of the outcomes are standardized) will need to set <code>autoAddIntercept</code> to <code>FALSE</code> and manually add the corresponding column vector in the data file, its block in <code>blockList</code> and its row/column in <code>SEMGraph</code> with <code>2</code>s on the row as required.</p>
<p>By setting <code>gammaInit</code> the user is able to decide how to initialise the gamma matrix, ginving the starting point of the MCMC.</p>
<p>This is achieved by passing “1” to get them all to start from 1 (meaning that all the variables will be included at the start), “0” (the opposite, only the compulsory variables will be included) or “R” for a random initialisation (care is needed if fixing the <code>seed</code> parameter in this last case).</p>
<p>The <code>method</code> parameter instead specify the sampler used for the binary matrices <span class="math inline">\(\Gamma_k\)</span>, with <code>method=0</code> being the traditional “<span class="math inline">\(MC^3\)</span>” Metropolis-Hastings sampler where at each iteration a certain number of coefficient are randomly proposed a switch to <code>0</code> or <code>1</code> (one at a time) and accepted/rejected according to the corresponding likelihood; <code>method=1</code> is a novel binary sampler by Banterle, Lewin (details and preprint available soon).</p>
<p><code>method=1</code> should be slightly slower in terms of computations but far more efficient in exploring the space.</p>
<p>These arguments have default values, meaning that if you want the intercepts to be added and to use the default (novel) sampler, there’s no need to specify them.</p>
</div>
</div>
<div id="software-output" class="section level2">
<h2>Software Output</h2>
<p>As of now the software outputs the average values of all the <span class="math inline">\(\Gamma_k\)</span> for the <em>optional</em> predictors of the first (untempered) chain in txt files called <code>[dataFileName]_HESS_gamma_[k]_out.txt</code> .</p>
<p>Each element of (each of) the matrix contains the posterior inclusion probability of the corresponding element of <span class="math inline">\(\Gamma_{k}\)</span> restricted to the <em>optional</em> variables.</p>
<p>Some other output files produced are:</p>
<ul>
<li><p><code>[dataFileName]_HESS_gamma_[k]_MCMC_out.txt</code> files, that contain the whole MCMC trace of <span class="math inline">\(\Gamma_{k}\)</span>, can be used to check convergence of the MCMC amongst other things.</p></li>
<li><p>one <code>[dataFileName]_HESS_logP_out.txt</code> file, that contains the overall log posterior probability at each step of the MCMC; This is also possibly an indicator of convergence (unless the chain got stuck in a local maximum) and should also be useful in selecting a possible best model.</p></li>
</ul>
</div>
<div id="running-the-software" class="section level2">
<h2>Running the software</h2>
<p>An example <code>R</code> code that generates data conformant to the above specifications, calls the main function and analyse the output is given below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mvtnorm)
<span class="kw">library</span>(MCMCpack)

## Do we want to generate data with missing values?
na=<span class="ot">TRUE</span>

## Simulate a 2 block SEM
n =<span class="st"> </span><span class="dv">200</span>
s_<span class="dv">1</span> =<span class="st"> </span><span class="dv">5</span>; s_<span class="dv">2</span> =<span class="st"> </span><span class="dv">3</span>; s=<span class="dv">8</span>
p =<span class="st"> </span><span class="dv">20</span>

## Sparsity -- simulate gamma matrices
sparsity_lvl =<span class="st"> </span><span class="fl">0.3</span>
gamma_<span class="dv">1</span> =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,p<span class="op">+</span><span class="dv">1</span>,s_<span class="dv">1</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>s_<span class="dv">1</span>){
  gamma_<span class="dv">1</span>[,i] =<span class="st"> </span><span class="kw">c</span>(<span class="ot">TRUE</span>, (<span class="dv">1</span><span class="op">:</span>p) <span class="op">%in%</span><span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>p,p<span class="op">*</span>sparsity_lvl,<span class="dt">replace =</span> <span class="ot">FALSE</span>) )
}

gamma_<span class="dv">2</span> =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,p<span class="op">+</span>s_<span class="dv">1</span><span class="op">+</span><span class="dv">1</span>,s_<span class="dv">2</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(s_<span class="dv">2</span>)){
  gamma_<span class="dv">2</span>[,i] =<span class="st"> </span><span class="kw">c</span>(<span class="ot">TRUE</span>, (<span class="dv">1</span><span class="op">:</span>(p<span class="op">+</span>s_<span class="dv">1</span>)) <span class="op">%in%</span><span class="st"> </span>
<span class="st">                    </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>(p<span class="op">+</span>s_<span class="dv">1</span>),(p<span class="op">+</span>s_<span class="dv">1</span>)<span class="op">*</span>sparsity_lvl,<span class="dt">replace =</span> <span class="ot">FALSE</span>) )
}

## Simulate some correlated Predictors ...
RX =<span class="st"> </span><span class="kw">riwish</span>(p<span class="op">+</span><span class="dv">5</span>, <span class="kw">riwish</span>(p<span class="op">+</span><span class="dv">5</span>, <span class="kw">diag</span>(p) ) )
RX =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">diag</span>(<span class="kw">sqrt</span>(<span class="kw">diag</span>(RX)))) <span class="op">%*%</span><span class="st"> </span>RX <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">diag</span>(<span class="kw">sqrt</span>(<span class="kw">diag</span>(RX))))  ## rescaled

x =<span class="st"> </span><span class="kw">rmvnorm</span>(n,<span class="kw">rep</span>(<span class="dv">0</span>,p),RX)
x =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n),x)

## .. and relative Regression Coefficients
sd_b =<span class="st"> </span><span class="dv">3</span>
b_<span class="dv">1</span> =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>((p<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>s_<span class="dv">1</span>,<span class="dv">5</span>,sd_b),p<span class="op">+</span><span class="dv">1</span>,s_<span class="dv">1</span>)
b_<span class="dv">2</span> =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>((p<span class="op">+</span>s_<span class="dv">1</span><span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>s_<span class="dv">2</span>,<span class="dv">5</span>,sd_b),p<span class="op">+</span>s_<span class="dv">1</span><span class="op">+</span><span class="dv">1</span>,s_<span class="dv">2</span>) 
  ## above we have both beta_2 AND lambda_2


## Residual variances and Errors
var_r_<span class="dv">1</span> =<span class="st"> </span><span class="kw">diag</span>(<span class="kw">rinvgamma</span>(s_<span class="dv">1</span>,<span class="fl">1.5</span>,<span class="dv">2</span>)); var_r_<span class="dv">2</span> =<span class="st"> </span><span class="kw">diag</span>(<span class="kw">rinvgamma</span>(s_<span class="dv">2</span>,<span class="fl">1.5</span>,<span class="dv">2</span>))
err_<span class="dv">1</span> =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>s_<span class="dv">1</span>),n,s_<span class="dv">1</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">chol</span>(var_r_<span class="dv">1</span>)
err_<span class="dv">2</span> =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>s_<span class="dv">2</span>),n,s_<span class="dv">2</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">chol</span>(var_r_<span class="dv">2</span>)

## Finally sample Ys
y=<span class="kw">matrix</span>(<span class="ot">NA</span>,n,s)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>s_<span class="dv">1</span>){
    y[,i] =<span class="st"> </span>x[,gamma_<span class="dv">1</span>[,i]] <span class="op">%*%</span><span class="st"> </span>b_<span class="dv">1</span>[gamma_<span class="dv">1</span>[,i],i]
  }
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>s_<span class="dv">2</span>){
  y[,i<span class="op">+</span>s_<span class="dv">1</span>] =<span class="st"> </span><span class="kw">cbind</span>(x,y[,<span class="dv">1</span><span class="op">:</span>s_<span class="dv">1</span>])[,gamma_<span class="dv">2</span>[,i]] <span class="op">%*%</span><span class="st"> </span>b_<span class="dv">2</span>[gamma_<span class="dv">2</span>[,i],i]
}

y =<span class="st"> </span>y <span class="op">+</span><span class="st"> </span><span class="kw">cbind</span>(err_<span class="dv">1</span>,err_<span class="dv">2</span>)

## Form the data matrix
data =<span class="st"> </span><span class="kw">cbind</span>(y,x[,<span class="op">-</span><span class="dv">1</span>])   <span class="co"># leave out the intercept because is coded inside already</span>

## Simulate some missing data
<span class="cf">if</span>(na){
  missing_rows =<span class="st"> </span><span class="kw">sample</span>( <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(data), <span class="fl">0.05</span><span class="op">*</span>n, <span class="dt">replace =</span> <span class="ot">FALSE</span> ) 
    ##~5% of the data's row will have missing values
  missing_idx =<span class="st"> </span><span class="kw">sample</span>( <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(data[missing_rows,]), 
          <span class="fl">0.1</span><span class="op">*</span><span class="kw">length</span>(data[missing_rows,]) , <span class="dt">replace =</span> <span class="ot">FALSE</span> ) 
    ##~10% of those data will be missing at random
  
  missing_data =<span class="st"> </span>(data[missing_rows,])[missing_idx] ## save them for later checking
  data[missing_rows,][missing_idx] =<span class="st"> </span><span class="ot">NaN</span>
}

#### Now build the software arguments

### list all the blocks
## starting from the Xs but that's arbitrary

blockL =<span class="st"> </span><span class="kw">list</span>( 
  <span class="kw">c</span>(<span class="dv">9</span><span class="op">:</span><span class="dv">28</span>),  ## x0 -- block 1
  <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>),  ## y1 -- block 2
  <span class="kw">c</span>(<span class="dv">6</span><span class="op">:</span><span class="dv">8</span>)  ## y2 -- block 3
)

### then the graph structure
## edge i,j means an arrow i -&gt; j

G =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>( 
  <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,
  <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,
  <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span> ), 
  <span class="dt">byrow=</span><span class="ot">TRUE</span>,<span class="dt">ncol=</span><span class="dv">3</span>,<span class="dt">nrow=</span><span class="dv">3</span>)

var_types =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,s_<span class="dv">1</span><span class="op">+</span>s_<span class="dv">2</span><span class="op">+</span>p)

## Write data to file and save the environment

<span class="cf">if</span>(na){
  <span class="kw">write.table</span>(<span class="dt">x=</span>data,<span class="dt">file=</span><span class="st">&quot;data/na_sem_data.txt&quot;</span>,<span class="dt">na =</span> <span class="st">&quot;NAN&quot;</span>,
              <span class="dt">col.names=</span><span class="ot">FALSE</span>,<span class="dt">row.names=</span><span class="ot">FALSE</span>)
  <span class="kw">save.image</span>(<span class="st">&quot;data/na_sample_data.RData&quot;</span>)
}<span class="cf">else</span>{
  <span class="kw">write.table</span>(<span class="dt">x=</span>data,<span class="dt">file=</span><span class="st">&quot;data/sem_data.txt&quot;</span>,<span class="dt">na =</span> <span class="st">&quot;NAN&quot;</span>,
              <span class="dt">col.names=</span><span class="ot">FALSE</span>,<span class="dt">row.names=</span><span class="ot">FALSE</span>)
  <span class="kw">save.image</span>(<span class="st">&quot;data/sample_data.RData&quot;</span>)
}

## Now call the function (assuming the rBSEM package is correctly installed)
nIter =<span class="st"> </span><span class="dv">20000</span>

<span class="cf">if</span>(<span class="op">!</span>na){
  <span class="kw">load</span>(<span class="st">&quot;data/sample_data.RData&quot;</span>)
  rBSEM<span class="op">::</span><span class="kw">rHESS_SEM</span>(<span class="dt">inFile=</span><span class="st">&quot;data/sem_data.txt&quot;</span>,<span class="dt">blockList =</span> blockL,
                   <span class="dt">SEMGraph =</span> G,<span class="dt">outFilePath=</span><span class="st">&quot;data/&quot;</span>,<span class="dt">nIter=</span>nIter, <span class="dt">method=</span><span class="dv">1</span>, <span class="dt">nChains =</span> <span class="dv">4</span>)
}<span class="cf">else</span>{
  <span class="kw">load</span>(<span class="st">&quot;data/na_sample_data.RData&quot;</span>)
  rBSEM<span class="op">::</span><span class="kw">rHESS_SEM</span>(<span class="dt">inFile=</span><span class="st">&quot;data/na_sem_data.txt&quot;</span>,<span class="dt">blockList =</span> blockL,
                   <span class="dt">SEMGraph =</span> G,<span class="dt">outFilePath=</span><span class="st">&quot;data/&quot;</span>,<span class="dt">nIter=</span>nIter, <span class="dt">method=</span><span class="dv">1</span>, <span class="dt">nChains =</span> <span class="dv">4</span>)
}


## then check some output
greyscale =<span class="st"> </span><span class="kw">grey</span>((<span class="dv">0</span><span class="op">:</span><span class="dv">1000</span>)<span class="op">/</span><span class="dv">1000</span>)

<span class="cf">if</span>(<span class="op">!</span>na){
  est_gamma_<span class="dv">1</span> =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">read.table</span>(<span class="st">&quot;data/sem_data_HESS_gamma_1_out.txt&quot;</span>) )
  est_gamma_<span class="dv">2</span> =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">read.table</span>(<span class="st">&quot;data/sem_data_HESS_gamma_2_out.txt&quot;</span>) )
}<span class="cf">else</span>{
  est_gamma_<span class="dv">1</span> =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">read.table</span>(<span class="st">&quot;data/na_sem_data_HESS_gamma_1_out.txt&quot;</span>) )
  est_gamma_<span class="dv">2</span> =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">read.table</span>(<span class="st">&quot;data/na_sem_data_HESS_gamma_2_out.txt&quot;</span>) )
}

## Note that there's correlation between X and y_1 so y_1 + x has some collienearity,
<span class="co"># which skews a bit the results on gamma_2</span>

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">image</span>(est_gamma_<span class="dv">1</span>,<span class="dt">col=</span>greyscale); <span class="kw">image</span>(gamma_<span class="dv">1</span>[<span class="op">-</span><span class="dv">1</span>,],<span class="dt">col=</span>greyscale)
<span class="kw">image</span>(est_gamma_<span class="dv">2</span>,<span class="dt">col=</span>greyscale); <span class="kw">image</span>(gamma_<span class="dv">2</span>[<span class="op">-</span><span class="dv">1</span>,],<span class="dt">col=</span>greyscale)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))

## check the behaviour of the logPosterior over the course of the chain
<span class="kw">plot</span>(<span class="kw">scan</span>(<span class="st">&quot;data/sem_data_HESS_logP_out.txt&quot;</span>),<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)

<span class="co"># maybe of just the second half?</span>
<span class="kw">plot</span>(<span class="kw">scan</span>(<span class="st">&quot;data/sem_data_HESS_logP_out.txt&quot;</span>)[(nIter<span class="op">/</span><span class="dv">2</span>)<span class="op">:</span>nIter],<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<p>In order to help in re-structuring the MCMC trace files to manageable array, the package contains a <code>traceToArray</code> function, whose documentation can be accessed via <code>?rBSEM::traceToArray</code>.</p>
<p>After the transformation the array should contain <code>nIterations</code> slices (<em>i.e.</em> elements accessed via <code>array[,,i]</code>), each with the same dimensions of <span class="math inline">\(\Gamma_k\)</span>.</p>
<p>An example use of it is given below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mcmc_gamma_<span class="dv">1</span> =<span class="st"> </span>rBSEM<span class="op">::</span><span class="kw">traceToArray</span>(<span class="dt">fileName =</span> <span class="st">&quot;data/sem_data_HESS_gamma_1_MCMC_out.txt&quot;</span>,<span class="dt">nIterations =</span> nIter)

<span class="co"># this below should return the same matrix as `est_gamma_1`</span>
<span class="kw">apply</span>( mcmc_gamma_<span class="dv">1</span> , <span class="dv">1</span><span class="op">:</span><span class="dv">2</span> , mean )</code></pre></div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
